{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2clmNgYO3iT",
        "outputId": "ab8ff771-9a77-4cca-99cb-5709ceace4e0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/gabrieldgf4/PlantVillage-Dataset.git data/PlantVillage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg1P3QglO_4W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.framework.ops import Tensor\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b-gs3l4Pa_S"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 25\n",
        "SEED=42\n",
        "INIT_LR = 1e-3\n",
        "BS = 32\n",
        "default_image_size = tuple((256, 256))\n",
        "image_size = 0\n",
        "train_path = 'train/'\n",
        "valid_path = 'val/'\n",
        "directory_root = 'data/PlantVillage/'\n",
        "width=256\n",
        "height=256\n",
        "depth=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyzpn1xtDOjx",
        "outputId": "3edf3ab0-28ac-4197-e25b-e1432aa6a8ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(directory_root,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BS,\n",
        "                                             image_size=default_image_size,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='training',\n",
        "                                             seed=SEED)\n",
        "validation_dataset =tf.keras.utils.image_dataset_from_directory(directory_root,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BS,\n",
        "                                             image_size=default_image_size,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='validation',\n",
        "                                             seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tvVXw5yDOjy",
        "outputId": "2c6fa70d-59e7-4e8e-c98b-9244ea7191bd"
      },
      "outputs": [],
      "source": [
        " \n",
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 64\n",
        "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    \n",
        "    zoom_range = 0.2, \n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True,\n",
        "    rotation_range = 180,\n",
        "    validation_split = 0.3) \n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    \n",
        "                                            directory_root,\n",
        "                                            class_mode = 'categorical',\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=64,\n",
        "                                             target_size =IMG_SIZE,\n",
        "                                             \n",
        "                                             subset='training',\n",
        "                                             seed=42)\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory( \n",
        "directory_root,\n",
        "                                            class_mode = 'categorical',\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=64,\n",
        "                                             target_size =IMG_SIZE,\n",
        "                                             \n",
        "                                             subset='validation',\n",
        "                                             seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VZ--yUJDOjz",
        "outputId": "754c91bd-8ac7-45a5-c4c7-98aabdbf7b26"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_generator.reset()\n",
        "x_train, y_train = next(train_generator)\n",
        "for i in tqdm(range(int(len(train_generator)/16-1))): #1st batch is already fetched before the for loop.\n",
        "  img, label = next(train_generator)\n",
        "  x_train = np.append(x_train, img, axis=0 )\n",
        "  y_train = np.append(y_train, label, axis=0)\n",
        "print(x_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mcOkNGJ5DOjz",
        "outputId": "adf36321-b10c-484f-a674-6a21b6d7a620"
      },
      "outputs": [],
      "source": [
        "\n",
        "validation_generator.reset()\n",
        "x_test, y_test = next(validation_generator)\n",
        "for i in tqdm(range(int(len(validation_generator)/8)-1)): #1st batch is already fetched before the for loop.\n",
        "  img, label = next(validation_generator)\n",
        "  x_test = np.append(x_test, img, axis=0 )\n",
        "  y_test = np.append(y_test, label, axis=0)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1lOgFqWoDOj0"
      },
      "outputs": [],
      "source": [
        "\n",
        "del(train_generator)\n",
        "del(validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LVgBxFk2DOj1",
        "outputId": "19ef537c-a015-46cd-ff51-5cb7d96a13b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "Y_train=[]\n",
        "for i in range(y_train.shape[0]):\n",
        "  temp=y_train[i][0:]\n",
        "  temp = np.argmax(temp)\n",
        "  Y_train.append(temp)\n",
        "\n",
        "Y_train=np.asarray(Y_train)\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G02RLVObDOj2",
        "outputId": "85338aab-0139-455d-9c8f-4b536abdfd34"
      },
      "outputs": [],
      "source": [
        "\n",
        "Y_test=[]\n",
        "for i in range(y_test.shape[0]):\n",
        "  temp=y_test[i][0:]\n",
        "  temp = np.argmax(temp)\n",
        "  Y_test.append(temp)\n",
        "\n",
        "Y_test=np.asarray(Y_test)\n",
        "Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFYRfj1KbhVr"
      },
      "outputs": [],
      "source": [
        "def data_augmenter():\n",
        "\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip('horizontal'))\n",
        "    data_augmentation.add(RandomRotation(0.2))\n",
        "    data_augmentation.add(RandomZoom(0.2))\n",
        "\n",
        "    return data_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JvUkDhCa7gcs"
      },
      "outputs": [],
      "source": [
        "input_shape = (256,256,3)\n",
        "model_input = Input(shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ilj41KNKTCmE"
      },
      "outputs": [],
      "source": [
        "def plotter(history, model):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  #Train and validation accuracy\n",
        "  plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "  #Train and validation loss\n",
        "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  print(\"[INFO] Calculating model accuracy\")\n",
        "  scores = model.evaluate(x_test, y_test)\n",
        "  print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sHSkIH53xJir"
      },
      "outputs": [],
      "source": [
        "def best_weights(preds1):\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame([])\n",
        "\n",
        "  for w1 in range(0, 5):\n",
        "      for w2 in range(0,5):\n",
        "              wts = [w1/10.,w2/10.]\n",
        "              wted_preds1 = np.tensordot(preds1, wts, axes=((0),(0)))\n",
        "              wted_ensemble_pred = np.argmax(wted_preds1, axis=1)\n",
        "              weighted_accuracy = accuracy_score(y_test, wted_ensemble_pred)\n",
        "              df = df.append(pd.DataFrame({'wt1':wts[0],'wt2':wts[1], 'acc':weighted_accuracy*100}, index=[0]), ignore_index=True)\n",
        "              \n",
        "  max_acc_row = df[df['acc']==df['acc'].max()]\n",
        "  print(\"Max accuracy of \", max_acc_row[0], \" obained with w1=\", max_acc_row[1],\n",
        "        \" w2=\", max_acc_row[2])         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eRP7uUc_cTBE"
      },
      "outputs": [],
      "source": [
        "def resnet(model_input: Tensor, data_augmentation=data_augmenter()):\n",
        "  base_model = tf.keras.applications.resnet50.ResNet50(input_shape=(256,256,3),\n",
        "                                                   include_top=False, \n",
        "                                                   weights='imagenet',input_tensor=model_input) \n",
        "\n",
        "  data_augmentation = data_augmenter()\n",
        "  x = data_augmentation(model_input)\n",
        "  x = tf.keras.applications.resnet50.preprocess_input(x) \n",
        "  x = base_model(x, training=False)  \n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "# Output as per the number of classes\n",
        "# Under Development\n",
        "  predictions = tf.keras.layers.Dense(40, activation='softmax')(x)\n",
        "  model = tf.keras.Model(inputs=model_input, outputs=predictions)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6X8RdF-Ec9jb"
      },
      "outputs": [],
      "source": [
        "def vgg():\n",
        "  base_model = tf.keras.applications.VGG16(input_shape=(256,256,3),\n",
        "                                                   include_top=False, \n",
        "                                                   weights='imagenet',input_tensor=model_input) \n",
        "  data_augmentation = data_augmenter()\n",
        "  x = data_augmentation(model_input)\n",
        "  x = tf.keras.applications.resnet50.preprocess_input(x) \n",
        "  x = base_model(x, training=False) \n",
        "\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  predictions = tf.keras.layers.Dense(40, activation='softmax')(x)\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PWJe-Rvadb8s",
        "outputId": "b452d20d-79cd-40a7-8bee-e72b0452fca5"
      },
      "outputs": [],
      "source": [
        "model1 = resnet(model_input, data_augmentation=data_augmenter())\n",
        "model1.summary()\n",
        "model1.compile(optimizer='adam', metrics=['accuracy'], loss='sparse_categorical_crossentropy')\n",
        "checkpoint = ModelCheckpoint('modified_resnet_50.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "history1 = model1.fit(\n",
        "    train_dataset,validation_data=validation_dataset,\n",
        "    steps_per_epoch=len(x_train) // BS,\n",
        "    epochs=EPOCHS, \n",
        "    callbacks=[checkpoint],\n",
        "    verbose=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_oe1TCSc9gT"
      },
      "outputs": [],
      "source": [
        "model2 = vgg().     \n",
        "model2.summary()\n",
        "model2.compile(optimizer='adam', metrics=['accuracy'], loss='sparse_categorical_crossentropy')\n",
        "checkpoint = ModelCheckpoint('modified_vgg_19.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "history2 = model2.fit(\n",
        "    train_dataset,validation_data=validation_dataset,\n",
        "    steps_per_epoch=len(x_train) // BS,\n",
        "    epochs=EPOCHS, \n",
        "    callbacks=[checkpoint],\n",
        "    verbose=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEXL8pZcfMVq"
      },
      "outputs": [],
      "source": [
        "plotter(history1, model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuocVF-sfRZW"
      },
      "outputs": [],
      "source": [
        "plotter(history2, model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOlOmY1pxzyO"
      },
      "outputs": [],
      "source": [
        "models = [model1, model2]\n",
        "\n",
        "preds = [model.predict(x_test) for model in models]\n",
        "preds=np.array(preds)\n",
        "summed = np.sum(preds, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IaZZYYXxs0I"
      },
      "outputs": [],
      "source": [
        "best_weights(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54-l3tIqfUZF"
      },
      "outputs": [],
      "source": [
        "ideal_weights = [0.7, 0.2]\n",
        "\n",
        "prediction1 = model1.predict_classes(x_test)\n",
        "prediction2 = model2.predict_classes(x_test)\n",
        "\n",
        "accuracy1 = accuracy_score(y_test, prediction1)\n",
        "accuracy2 = accuracy_score(y_test, prediction2)\n",
        "\n",
        "ideal_weighted_preds = np.tensordot(preds, ideal_weights, axes=((0),(0)))\n",
        "ideal_weighted_ensemble_prediction = np.argmax(ideal_weighted_preds, axis=1)\n",
        "\n",
        "ideal_weighted_accuracy = accuracy_score(y_test, ideal_weighted_ensemble_prediction)\n",
        "\n",
        "print('Accuracy Score for model1 = ', accuracy1)\n",
        "print('Accuracy Score for model2 = ', accuracy2)\n",
        "print('Accuracy Score for average ensemble = ', ideal_weighted_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tensorflow_venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "22919348e4495772b891b8f26ea421fd2aa751dd1ac0ab96942fcce54cacab0e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
